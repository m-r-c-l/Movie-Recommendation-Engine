{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7801a195",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622aabcc",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9f99aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7849f16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48bf9d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-06 13:21:45.956928: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-06 13:21:45.998006: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-06 13:21:45.998061: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-06 13:21:45.999981: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-06 13:21:46.009093: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-06 13:21:46.010628: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-06 13:21:47.336931: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/marcel/.pyenv/versions/3.10.6/envs/Movie-Recommendation-Engine/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_recommenders as tfrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5f50c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1eeeed37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33771774",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f546c13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8469e27b",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10b20fdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/marcel/code/m-r-c-l/Movie-Recommendation-Engine/raw_data'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_directory = os.getcwd()\n",
    "data_directory = os.path.join(current_directory, '..', 'raw_data')\n",
    "data_directory = os.path.abspath(data_directory)\n",
    "data_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "480bf2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processed_path_movies = \"/home/marcel/code/m-r-c-l/Movie-Recommendation-Engine/raw_data/movies_new_df.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de33f1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processed_path_ratings = \"/home/marcel/code/m-r-c-l/Movie-Recommendation-Engine/raw_data/ratings_new_df.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22720df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/marcel/.pyenv/versions/3.10.6/envs/Movie-Recommendation-Engine/lib/python3.10/site-packages/tensorflow/python/data/experimental/ops/readers.py:573: ignore_errors (from tensorflow.python.data.experimental.ops.error_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.ignore_errors` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-06 13:21:52.271052: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-09-06 13:21:52.271509: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2256] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "ds_movies = tf.data.experimental.make_csv_dataset(\n",
    "    data_processed_path_movies,\n",
    "    batch_size=4096,\n",
    "    header=True,\n",
    "    #column_names=list(df_small.columns),\n",
    "    #label_name=65,\n",
    "    #num_epochs=1,\n",
    "    ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9c5c84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_ratings = tf.data.experimental.make_csv_dataset(\n",
    "    data_processed_path_ratings,\n",
    "    batch_size=4096,\n",
    "    header=True,\n",
    "    #column_names=list(df_small.columns),\n",
    "    #label_name=65,\n",
    "    #num_epochs=1,\n",
    "    ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9844c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load each CSV file into a DataFrame\n",
    "#links_df = pd.read_csv(os.path.join(data_directory, 'ml-latest-small/links.csv'))\n",
    "#ratings_df = pd.read_csv(os.path.join(data_directory, 'ml-latest-small/ratings.csv'))\n",
    "#tags_df = pd.read_csv(os.path.join(data_directory, 'ml-latest-small/tags.csv'))\n",
    "#movies_df = pd.read_csv(os.path.join(data_directory, 'ml-latest-small/movies.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3baa137",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027c3923",
   "metadata": {},
   "source": [
    "### Exploring ' and \" to see if that could be an issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2e1fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#double_quote_df = merged_df[merged_df['movie_title'].str.contains(r'\"')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696b33c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#single_quote_df = merged_df[merged_df['movie_title'].str.contains(r\"'\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fcd7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#double_quote_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f237417",
   "metadata": {},
   "outputs": [],
   "source": [
    "#single_quote_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0fd7dc",
   "metadata": {},
   "source": [
    "### Convert timestamp data into datetime data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576cab89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ratings_df['date'] = ratings_df['timestamp'].apply(lambda x: datetime.fromtimestamp(x))\n",
    "#ratings_df.drop('timestamp', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8496e65a",
   "metadata": {},
   "source": [
    "### Merge movies_df with ratings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7e9956",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Merge movies_df with ratings_df\n",
    "#merged_df = ratings_df.merge(movies_df[['movieId', 'title', 'genres']], left_on='movieId',right_on='movieId', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fe1bc2",
   "metadata": {},
   "source": [
    "### Convert user and title to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e97247",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merged_df['userId'] = merged_df['userId'].astype(str)\n",
    "#merged_df['title'] = merged_df['title'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697a8813",
   "metadata": {},
   "source": [
    "### Clean title data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d3763b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def clean_title(title):\n",
    "#    # Removing year in parentheses\n",
    "#    title = re.sub(r'\\(\\d{4}\\)', '', title)\n",
    "#    # Removing parentheses\n",
    "#    title = title.replace('()', '')\n",
    "#    # Striping extra spaces\n",
    "#    return title.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f9079b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merged_df['movie_title'] = merged_df['title'].apply(clean_title)\n",
    "#merged_df.drop('title', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4902209e",
   "metadata": {},
   "source": [
    "### Rename columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364a4b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merged_df = merged_df.rename(columns={'userId': 'user_id', 'movieId': 'movie_id'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c987ce7d",
   "metadata": {},
   "source": [
    "### Convert df into tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a019078",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#data_dict = {\n",
    "#    'user_id': tf.convert_to_tensor(merged_df['user_id'].values),\n",
    "#    'rating': tf.convert_to_tensor(merged_df['rating'].values),\n",
    "#    'movie_title': tf.convert_to_tensor(merged_df['movie_title'].values)\n",
    "#}\n",
    "#\n",
    "#dataset = tf.data.Dataset.from_tensor_slices(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71dd53a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the basic features.\n",
    "ratings = ds_ratings.map(lambda x: {\n",
    "    \"movie_title\": x[\"movieId\"],\n",
    "    \"user_id\": x[\"userId\"]\n",
    "})\n",
    "\n",
    "movies = ds_movies.map(lambda x: x[\"movieId\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd6604c",
   "metadata": {},
   "source": [
    "#### Validate that the transformation worked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45187b55",
   "metadata": {},
   "source": [
    "##### How ratings should look like:\n",
    "\n",
    "- Source: https://colab.research.google.com/github/tensorflow/recommenders/blob/main/docs/examples/basic_retrieval.ipynb#scrollTo=_1-KQV2ynMdh\n",
    "\n",
    "for x in ratings.take(1).as_numpy_iterator():\n",
    "  pprint.pprint(x)\n",
    "\n",
    "{'bucketized_user_age': 45.0,\n",
    " 'movie_genres': array([7]),\n",
    " 'movie_id': b'357',\n",
    " 'movie_title': b\"One Flew Over the Cuckoo's Nest (1975)\",\n",
    " 'raw_user_age': 46.0,\n",
    " 'timestamp': 879024327,\n",
    " 'user_gender': True,\n",
    " 'user_id': b'138',\n",
    " 'user_occupation_label': 4,\n",
    " 'user_occupation_text': b'doctor',\n",
    " 'user_rating': 4.0,\n",
    " 'user_zip_code': b'53211'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1594a940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'movie_title': array([ 3977,   508,  4014, ...,  1371,  1682, 51540], dtype=int32),\n",
      " 'user_id': array([62,  6, 63, ..., 19, 24, 29], dtype=int32)}\n"
     ]
    }
   ],
   "source": [
    "for x in ratings.take(1).as_numpy_iterator():\n",
    "  pprint.pprint(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078aac7f",
   "metadata": {},
   "source": [
    "##### How movies should look like:\n",
    "\n",
    "- Source: https://colab.research.google.com/github/tensorflow/recommenders/blob/main/docs/examples/basic_retrieval.ipynb#scrollTo=_1-KQV2ynMdh\n",
    " \n",
    "for x in movies.take(1).as_numpy_iterator():\n",
    "  pprint.pprint(x)\n",
    "\n",
    "{'movie_genres': array([4]),\n",
    " 'movie_id': b'1681',\n",
    " 'movie_title': b'You So Crazy (1994)'}       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d221bf76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([  5415,   2761,   2058, ..., 170993,   2383, 172497], dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for x in movies.take(1).as_numpy_iterator():\n",
    "  pprint.pprint(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049ccf4b",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### MISC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e49f64c",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for movie in movies.take(1):\n",
    "    print(movie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3506bd",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for rating in ratings.take(1):\n",
    "    print(rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e25bb3",
   "metadata": {},
   "source": [
    "# The models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e012566d",
   "metadata": {},
   "source": [
    "## Model I (based on TensorFlow tutorial - with our data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668b9f6b",
   "metadata": {},
   "source": [
    "### Train, test, split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb85a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "shuffled = ratings.shuffle(100, seed=42, reshuffle_each_iteration=False) ## shuffles the entire data set once\n",
    "\n",
    "train = shuffled.take(80_000)\n",
    "test = shuffled.skip(80_000).take(20_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c26b41",
   "metadata": {},
   "source": [
    "#### Check len of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0332b389",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_rows = len(list(shuffled))\n",
    "num_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664546fc",
   "metadata": {},
   "source": [
    "### Create batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccd0702",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_titles = movies.batch(1_000)\n",
    "user_ids = ratings.batch(1_000).map(lambda x: x[\"user_id\"]) #### 1m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511cb06b",
   "metadata": {},
   "source": [
    "### Get unique user_ids and movie_titles "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf1322d",
   "metadata": {},
   "source": [
    "This is important because we need to be able to map the raw values of our categorical features to embedding vectors in our models. To do that, we need a vocabulary that maps a raw feature value to an integer in a contiguous range: this allows us to look up the corresponding embeddings in our embedding tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc68d02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_movie_titles = np.unique(np.concatenate(list(movie_titles)))\n",
    "unique_user_ids = np.unique(np.concatenate(list(user_ids)))\n",
    "\n",
    "unique_movie_titles[100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebbaf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Unique Movies: {}'.format(len(unique_movie_titles)))\n",
    "print('Unique users: {}'.format(len(unique_user_ids)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ac0702",
   "metadata": {},
   "source": [
    "### Model setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436bb7d1",
   "metadata": {},
   "source": [
    "Choosing the architecture of our model is a key part of modelling.\n",
    "\n",
    "Because we are building a two-tower retrieval model, we can build each tower separately and then combine them in the final model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5130a2",
   "metadata": {},
   "source": [
    "#### The query tower"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7392dc",
   "metadata": {},
   "source": [
    "The first step is to decide on the dimensionality of the query and candidate representations. Higher values will correspond to models that may be more accurate, but will also be slower to fit and more prone to overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d891e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dimension = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82c057f",
   "metadata": {},
   "source": [
    "The second is to define the model itself. Here, we're going to use Keras preprocessing layers to first convert user ids to integers, and then convert those to user embeddings via an Embedding layer. Note that we use the list of unique user ids we computed earlier as a vocabulary:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa41eebe",
   "metadata": {},
   "source": [
    "A simple model like this corresponds exactly to a classic matrix factorization approach. While defining a subclass of tf.keras.Model for this simple model might be overkill, we can easily extend it to an arbitrarily complex model using standard Keras components, as long as we return an embedding_dimension-wide output at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fd97c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_model = tf.keras.Sequential([\n",
    "  tf.keras.layers.StringLookup(\n",
    "      vocabulary=unique_user_ids, mask_token=None),\n",
    "  # We add an additional embedding to account for unknown tokens.\n",
    "  tf.keras.layers.Embedding(len(unique_user_ids) + 1, embedding_dimension)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a0cebf",
   "metadata": {},
   "source": [
    "#### The candidate tower"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4d9376",
   "metadata": {},
   "source": [
    "We can do the same with the candidate tower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e2671c",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_model = tf.keras.Sequential([\n",
    "  tf.keras.layers.StringLookup(\n",
    "      vocabulary=unique_movie_titles, mask_token=None),\n",
    "  tf.keras.layers.Embedding(len(unique_movie_titles) + 1, embedding_dimension)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfef31d",
   "metadata": {},
   "source": [
    "#### Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94107fee",
   "metadata": {},
   "source": [
    "In our training data we have positive (user, movie) pairs. To figure out how good our model is, we need to compare the affinity score that the model calculates for this pair to the scores of all the other possible candidates: if the score for the positive pair is higher than for all other candidates, our model is highly accurate.\n",
    "\n",
    "To do this, we can use the tfrs.metrics.FactorizedTopK metric. The metric has one required argument: the dataset of candidates that are used as implicit negatives for evaluation.\n",
    "\n",
    "In our case, that's the movies dataset, converted into embeddings via our movie model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ed5d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = tfrs.metrics.FactorizedTopK(\n",
    "  candidates=movies.batch(128).map(movie_model)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5bc414",
   "metadata": {},
   "source": [
    "#### Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e264a2c",
   "metadata": {},
   "source": [
    "The next component is the loss used to train our model. TFRS has several loss layers and tasks to make this easy.\n",
    "\n",
    "In this instance, we'll make use of the Retrieval task object: a convenience wrapper that bundles together the loss function and metric computation.\n",
    "\n",
    "The task itself is a Keras layer that takes the query and candidate embeddings as arguments, and returns the computed loss: we'll use that to implement the model's training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c598c9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = tfrs.tasks.Retrieval(\n",
    "  metrics=metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328820be",
   "metadata": {},
   "source": [
    "#### The full model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e431539",
   "metadata": {},
   "source": [
    "We can now put it all together into a model. TFRS exposes a base model class (tfrs.models.Model) which streamlines building models: all we need to do is to set up the components in the __init__ method, and implement the compute_loss method, taking in the raw features and returning a loss value.\n",
    "\n",
    "The base model will then take care of creating the appropriate training loop to fit our model.\n",
    "\n",
    "The tfrs.Model base class is a simply convenience class: it allows us to compute both training and test losses using the same method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9da29d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovielensModel(tfrs.Model):\n",
    "\n",
    "  def __init__(self, user_model, movie_model):\n",
    "    super().__init__()\n",
    "    self.movie_model: tf.keras.Model = movie_model\n",
    "    self.user_model: tf.keras.Model = user_model\n",
    "    self.task: tf.keras.layers.Layer = task\n",
    "\n",
    "  def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
    "    # We pick out the user features and pass them into the user model.\n",
    "    user_embeddings = self.user_model(features[\"user_id\"])\n",
    "    # And pick out the movie features and pass them into the movie model,\n",
    "    # getting embeddings back.\n",
    "    positive_movie_embeddings = self.movie_model(features[\"movie_title\"])\n",
    "\n",
    "    # The task computes the loss and the metrics.\n",
    "    return self.task(user_embeddings, positive_movie_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b759e2",
   "metadata": {},
   "source": [
    "### Fitting and evaluating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdf922d",
   "metadata": {},
   "source": [
    "After defining the model, we can use standard Keras fitting and evaluation routines to fit and evaluate the model.\n",
    "\n",
    "Let's first instantiate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1457a8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MovielensModel(user_model, movie_model)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b261ab6c",
   "metadata": {},
   "source": [
    "Then shuffle, batch, and cache the training and evaluation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc768eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_train = train.shuffle(100_000).batch(8192).cache()\n",
    "cached_test = test.batch(4096).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e52d37",
   "metadata": {},
   "source": [
    "Then train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e49b8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(cached_train, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3521e3",
   "metadata": {},
   "source": [
    "If you want to monitor the training process with TensorBoard, you can add a TensorBoard callback to fit() function and then start TensorBoard using %tensorboard --logdir logs/fit. Please refer to TensorBoard documentation for more details.\n",
    "\n",
    "As the model trains, the loss is falling and a set of top-k retrieval metrics is updated. These tell us whether the true positive is in the top-k retrieved items from the entire candidate set. For example, a top-5 categorical accuracy metric of 0.2 would tell us that, on average, the true positive is in the top 5 retrieved items 20% of the time.\n",
    "\n",
    "Note that, in this example, we evaluate the metrics during training as well as evaluation. Because this can be quite slow with large candidate sets, it may be prudent to turn metric calculation off in training, and only run it in evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c58fb32",
   "metadata": {},
   "source": [
    "Finally, we can evaluate our model on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed9932f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.evaluate(cached_test, return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f78cd5",
   "metadata": {},
   "source": [
    "Test set performance is much worse than training performance. This is due to two factors:\n",
    "\n",
    "1. Our model is likely to perform better on the data that it has seen, simply because it can memorize it. This overfitting phenomenon is especially strong when models have many parameters. It can be mediated by model regularization and use of user and movie features that help the model generalize better to unseen data.\n",
    "2. The model is re-recommending some of users' already watched movies. These known-positive watches can crowd out test movies out of top K recommendations.\n",
    "\n",
    "The second phenomenon can be tackled by excluding previously seen movies from test recommendations. This approach is relatively common in the recommender systems literature, but we don't follow it in these tutorials. If not recommending past watches is important, we should expect appropriately specified models to learn this behaviour automatically from past user history and contextual information. Additionally, it is often appropriate to recommend the same item multiple times (say, an evergreen TV series or a regularly purchased item)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65759229",
   "metadata": {},
   "source": [
    "## Model II (based on TensorFlow tutorial - with Tensorflow data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e813f1",
   "metadata": {},
   "source": [
    "### Load data from tutorial (instead of using our own data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824ab3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ratings data.\n",
    "ratings = tfds.load(\"movielens/25m-ratings\", split=\"train\")\n",
    "# Features of all the available movies.\n",
    "movies = tfds.load(\"movielens/25m-movies\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45833a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = ratings.map(lambda x: {\n",
    "    \"movie_title\": x[\"movie_title\"],\n",
    "    \"user_id\": x[\"user_id\"],\n",
    "})\n",
    "movies = movies.map(lambda x: x[\"movie_title\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96dcfb2",
   "metadata": {},
   "source": [
    "### Train, test, split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a8c3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "shuffled = ratings.shuffle(100_000, seed=42, reshuffle_each_iteration=False) ## shuffles the entire data set once\n",
    "\n",
    "train = shuffled.take(80_000)\n",
    "test = shuffled.skip(80_000).take(20_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4b4b8a",
   "metadata": {},
   "source": [
    "#### Check len of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fb7604",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_rows = len(list(shuffled))\n",
    "num_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ced3ccb",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Create batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7d92be",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "movie_titles = movies.batch(1_000)\n",
    "user_ids = ratings.batch(1_000_000).map(lambda x: x[\"user_id\"]) #### 1m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300fffe6",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Get unique user_ids and movie_titles "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb54c54",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This is important because we need to be able to map the raw values of our categorical features to embedding vectors in our models. To do that, we need a vocabulary that maps a raw feature value to an integer in a contiguous range: this allows us to look up the corresponding embeddings in our embedding tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b917325",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "unique_movie_titles = np.unique(np.concatenate(list(movie_titles)))\n",
    "unique_user_ids = np.unique(np.concatenate(list(user_ids)))\n",
    "\n",
    "unique_movie_titles[100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c552b67e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print('Unique Movies: {}'.format(len(unique_movie_titles)))\n",
    "print('Unique users: {}'.format(len(unique_user_ids)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b3348a",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Model setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf9aac4",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Choosing the architecture of our model is a key part of modelling.\n",
    "\n",
    "Because we are building a two-tower retrieval model, we can build each tower separately and then combine them in the final model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00cce9a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### The query tower"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad0171c",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The first step is to decide on the dimensionality of the query and candidate representations. Higher values will correspond to models that may be more accurate, but will also be slower to fit and more prone to overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b00583",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "embedding_dimension = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdca313",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The second is to define the model itself. Here, we're going to use Keras preprocessing layers to first convert user ids to integers, and then convert those to user embeddings via an Embedding layer. Note that we use the list of unique user ids we computed earlier as a vocabulary:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2b2deb",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A simple model like this corresponds exactly to a classic matrix factorization approach. While defining a subclass of tf.keras.Model for this simple model might be overkill, we can easily extend it to an arbitrarily complex model using standard Keras components, as long as we return an embedding_dimension-wide output at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b3ea1a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "user_model = tf.keras.Sequential([\n",
    "  tf.keras.layers.StringLookup(\n",
    "      vocabulary=unique_user_ids, mask_token=None),\n",
    "  # We add an additional embedding to account for unknown tokens.\n",
    "  tf.keras.layers.Embedding(len(unique_user_ids) + 1, embedding_dimension)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b87528d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### The candidate tower"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3edfd3",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can do the same with the candidate tower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da25e6c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "movie_model = tf.keras.Sequential([\n",
    "  tf.keras.layers.StringLookup(\n",
    "      vocabulary=unique_movie_titles, mask_token=None),\n",
    "  tf.keras.layers.Embedding(len(unique_movie_titles) + 1, embedding_dimension)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dca60aa",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad38d22",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In our training data we have positive (user, movie) pairs. To figure out how good our model is, we need to compare the affinity score that the model calculates for this pair to the scores of all the other possible candidates: if the score for the positive pair is higher than for all other candidates, our model is highly accurate.\n",
    "\n",
    "To do this, we can use the tfrs.metrics.FactorizedTopK metric. The metric has one required argument: the dataset of candidates that are used as implicit negatives for evaluation.\n",
    "\n",
    "In our case, that's the movies dataset, converted into embeddings via our movie model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd53d0eb",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "metrics = tfrs.metrics.FactorizedTopK(\n",
    "  candidates=movies.batch(128).map(movie_model)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd82282",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e360c9",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The next component is the loss used to train our model. TFRS has several loss layers and tasks to make this easy.\n",
    "\n",
    "In this instance, we'll make use of the Retrieval task object: a convenience wrapper that bundles together the loss function and metric computation.\n",
    "\n",
    "The task itself is a Keras layer that takes the query and candidate embeddings as arguments, and returns the computed loss: we'll use that to implement the model's training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ada7aab",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "task = tfrs.tasks.Retrieval(\n",
    "  metrics=metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234c0fe7",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### The full model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f83daf",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can now put it all together into a model. TFRS exposes a base model class (tfrs.models.Model) which streamlines building models: all we need to do is to set up the components in the __init__ method, and implement the compute_loss method, taking in the raw features and returning a loss value.\n",
    "\n",
    "The base model will then take care of creating the appropriate training loop to fit our model.\n",
    "\n",
    "The tfrs.Model base class is a simply convenience class: it allows us to compute both training and test losses using the same method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16bfe8a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class MovielensModel(tfrs.Model):\n",
    "\n",
    "  def __init__(self, user_model, movie_model):\n",
    "    super().__init__()\n",
    "    self.movie_model: tf.keras.Model = movie_model\n",
    "    self.user_model: tf.keras.Model = user_model\n",
    "    self.task: tf.keras.layers.Layer = task\n",
    "\n",
    "  def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
    "    # We pick out the user features and pass them into the user model.\n",
    "    user_embeddings = self.user_model(features[\"user_id\"])\n",
    "    # And pick out the movie features and pass them into the movie model,\n",
    "    # getting embeddings back.\n",
    "    positive_movie_embeddings = self.movie_model(features[\"movie_title\"])\n",
    "\n",
    "    # The task computes the loss and the metrics.\n",
    "    return self.task(user_embeddings, positive_movie_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd491d5",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Fitting and evaluating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d6b39c",
   "metadata": {
    "hidden": true
   },
   "source": [
    "After defining the model, we can use standard Keras fitting and evaluation routines to fit and evaluate the model.\n",
    "\n",
    "Let's first instantiate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0de20a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = MovielensModel(user_model, movie_model)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0447b501",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Then shuffle, batch, and cache the training and evaluation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d1bb8a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cached_train = train.shuffle(100_000).batch(8192).cache()\n",
    "cached_test = test.batch(4096).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec2a1e7",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Then train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e41d795",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.fit(cached_train, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969da83f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "If you want to monitor the training process with TensorBoard, you can add a TensorBoard callback to fit() function and then start TensorBoard using %tensorboard --logdir logs/fit. Please refer to TensorBoard documentation for more details.\n",
    "\n",
    "As the model trains, the loss is falling and a set of top-k retrieval metrics is updated. These tell us whether the true positive is in the top-k retrieved items from the entire candidate set. For example, a top-5 categorical accuracy metric of 0.2 would tell us that, on average, the true positive is in the top 5 retrieved items 20% of the time.\n",
    "\n",
    "Note that, in this example, we evaluate the metrics during training as well as evaluation. Because this can be quite slow with large candidate sets, it may be prudent to turn metric calculation off in training, and only run it in evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8e2c46",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Finally, we can evaluate our model on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0314d76",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#model.evaluate(cached_test, return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2a867d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Test set performance is much worse than training performance. This is due to two factors:\n",
    "\n",
    "1. Our model is likely to perform better on the data that it has seen, simply because it can memorize it. This overfitting phenomenon is especially strong when models have many parameters. It can be mediated by model regularization and use of user and movie features that help the model generalize better to unseen data.\n",
    "2. The model is re-recommending some of users' already watched movies. These known-positive watches can crowd out test movies out of top K recommendations.\n",
    "\n",
    "The second phenomenon can be tackled by excluding previously seen movies from test recommendations. This approach is relatively common in the recommender systems literature, but we don't follow it in these tutorials. If not recommending past watches is important, we should expect appropriately specified models to learn this behaviour automatically from past user history and contextual information. Additionally, it is often appropriate to recommend the same item multiple times (say, an evergreen TV series or a regularly purchased item)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace851fd",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Making predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de105911",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now that we have a model, we would like to be able to make predictions. We can use the tfrs.layers.factorized_top_k.BruteForce layer to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75fbadc",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create a model that takes in raw query features, and\n",
    "index = tfrs.layers.factorized_top_k.BruteForce(model.user_model)\n",
    "# recommends movies out of the entire movies dataset.\n",
    "index.index_from_dataset(\n",
    "  tf.data.Dataset.zip((movies.batch(100), movies.batch(100).map(model.movie_model)))\n",
    ")\n",
    "\n",
    "# Get recommendations.\n",
    "_, titles = index(tf.constant([\"42\"]))\n",
    "print(f\"Recommendations for user 42: {titles[0, :3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7159599c",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Deploying the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b113a608",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Here's an interesting section in the tutorial that talk about how to make predictions faster and more efficient: https://www.tensorflow.org/recommenders/examples/basic_retrieval#model_serving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd95d31",
   "metadata": {},
   "source": [
    "## Model III (based on Kaggle example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f9c507",
   "metadata": {},
   "source": [
    "### Import dataset from tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fceb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ratings data.\n",
    "ratings = tfds.load(\"movielens/100k-ratings\", split=\"train\")\n",
    "# Features of all the available movies.\n",
    "movies = tfds.load(\"movielens/100k-movies\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34b2533",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183cc804",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = ratings.map(lambda x: {\n",
    "    \"movie_title\": x[\"movie_title\"],\n",
    "    \"user_id\": x[\"user_id\"],\n",
    "    \"user_ratings\": float(x[\"user_rating\"])\n",
    "})\n",
    "movies = movies.map(lambda x: x[\"movie_title\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a3b169",
   "metadata": {},
   "source": [
    "### General model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7f0b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the basic features (including rating now)\n",
    "ratings = dataset.map(lambda x: {\n",
    "    \"movie_title\": x[\"movie_title\"],\n",
    "    \"user_id\": x[\"user_id\"],\n",
    "    \"rating\": float(x[\"rating\"])\n",
    "})\n",
    "\n",
    "movies = dataset.map(lambda x: x[\"movie_title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba07171",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Total Data: {}'.format(len(ratings)))\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "shuffled = ratings.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
    "\n",
    "train = ratings.take(80_000)\n",
    "test = ratings.skip(80_000).take(20_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f259b920",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "movie_titles = movies.batch(1_000)\n",
    "user_ids = ratings.batch(1_000).map(lambda x: x[\"user_id\"])\n",
    "\n",
    "unique_movie_titles = np.unique(np.concatenate(list(movie_titles)))\n",
    "unique_user_ids = np.unique(np.concatenate(list(user_ids)))\n",
    "\n",
    "print('Unique Movies: {}'.format(len(unique_movie_titles)))\n",
    "print('Unique users: {}'.format(len(unique_user_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecc3d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieModel(tfrs.models.Model):\n",
    "\n",
    "  def __init__(self, rating_weight: float, retrieval_weight: float) -> None:\n",
    "    # We take the loss weights in the constructor: this allows us to instantiate\n",
    "    # several model objects with different loss weights.\n",
    "\n",
    "    super().__init__()\n",
    "\n",
    "    embedding_dimension = 64\n",
    "\n",
    "    # User and movie models.\n",
    "    self.movie_model: tf.keras.layers.Layer = tf.keras.Sequential([\n",
    "      tf.keras.layers.StringLookup(\n",
    "        vocabulary=unique_movie_titles, mask_token=None),\n",
    "      tf.keras.layers.Embedding(len(unique_movie_titles) + 1, embedding_dimension)\n",
    "    ])\n",
    "    self.user_model: tf.keras.layers.Layer = tf.keras.Sequential([\n",
    "      tf.keras.layers.StringLookup(\n",
    "        vocabulary=unique_user_ids, mask_token=None),\n",
    "      tf.keras.layers.Embedding(len(unique_user_ids) + 1, embedding_dimension)\n",
    "    ])\n",
    "\n",
    "    # A small model to take in user and movie embeddings and predict ratings.\n",
    "    # We can make this as complicated as we want as long as we output a scalar\n",
    "    # as our prediction.\n",
    "    self.rating_model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(256, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(1),\n",
    "    ])\n",
    "\n",
    "    # The tasks.\n",
    "    self.rating_task: tf.keras.layers.Layer = tfrs.tasks.Ranking(\n",
    "        loss=tf.keras.losses.MeanSquaredError(),\n",
    "        metrics=[tf.keras.metrics.RootMeanSquaredError()],\n",
    "    )\n",
    "    self.retrieval_task: tf.keras.layers.Layer = tfrs.tasks.Retrieval(\n",
    "        metrics=tfrs.metrics.FactorizedTopK(\n",
    "            candidates=movies.batch(128).map(self.movie_model)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # The loss weights.\n",
    "    self.rating_weight = rating_weight\n",
    "    self.retrieval_weight = retrieval_weight\n",
    "\n",
    "  def call(self, features: Dict[Text, tf.Tensor]) -> tf.Tensor:\n",
    "    # We pick out the user features and pass them into the user model.\n",
    "    user_embeddings = self.user_model(features[\"user_id\"])\n",
    "    # And pick out the movie features and pass them into the movie model.\n",
    "    movie_embeddings = self.movie_model(features[\"movie_title\"])\n",
    "    \n",
    "    return (\n",
    "        user_embeddings,\n",
    "        movie_embeddings,\n",
    "        # We apply the multi-layered rating model to a concatentation of\n",
    "        # user and movie embeddings.\n",
    "        self.rating_model(\n",
    "            tf.concat([user_embeddings, movie_embeddings], axis=1)\n",
    "        ),\n",
    "    )\n",
    "\n",
    "  def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
    "\n",
    "    ratings = features.pop(\"user_ratings\")\n",
    "\n",
    "    user_embeddings, movie_embeddings, rating_predictions = self(features)\n",
    "\n",
    "    # We compute the loss for each task.\n",
    "    rating_loss = self.rating_task(\n",
    "        labels=ratings,\n",
    "        predictions=rating_predictions,\n",
    "    )\n",
    "    retrieval_loss = self.retrieval_task(user_embeddings, movie_embeddings)\n",
    "\n",
    "    # And combine them using the loss weights.\n",
    "    return (self.rating_weight * rating_loss\n",
    "            + self.retrieval_weight * retrieval_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2b197f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MovieModel(rating_weight=1.0, retrieval_weight=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601b8454",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))\n",
    "\n",
    "cached_train = train.shuffle(100_000).batch(1_000).cache()\n",
    "cached_test = test.batch(1_000).cache()\n",
    "\n",
    "model.fit(cached_train, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77eea52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = model.evaluate(cached_test, return_dict=True)\n",
    "\n",
    "print(f\"\\nRetrieval top-100 accuracy: {metrics['factorized_top_k/top_100_categorical_accuracy']:.3f}\")\n",
    "print(f\"Ranking RMSE: {metrics['root_mean_squared_error']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bbb4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### model.save_weights('tfrs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880e633a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_movie(user, top_n=3):\n",
    "    # Create a model that takes in raw query features, and\n",
    "    index = tfrs.layers.factorized_top_k.BruteForce(model.user_model)\n",
    "    # recommends movies out of the entire movies dataset.\n",
    "    index.index_from_dataset(\n",
    "      tf.data.Dataset.zip((movies.batch(100), movies.batch(100).map(model.movie_model)))\n",
    "    )\n",
    "\n",
    "    # Get recommendations.\n",
    "    _, titles = index(tf.constant([str(user)]))\n",
    "    \n",
    "    print('Top {} recommendations for user {}:\\n'.format(top_n, user))\n",
    "    for i, title in enumerate(titles[0, :top_n].numpy()):\n",
    "        print('{}. {}'.format(i+1, title.decode(\"utf-8\")))\n",
    "\n",
    "def predict_rating(user, movie):\n",
    "    trained_movie_embeddings, trained_user_embeddings, predicted_rating = model({\n",
    "          \"userId\": np.array([str(user)]),\n",
    "          \"original_title\": np.array([movie])\n",
    "      })\n",
    "    print(\"Predicted rating for {}: {}\".format(movie, predicted_rating.numpy()[0][0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
